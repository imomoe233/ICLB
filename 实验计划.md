触发器从shadow中产生，通过cifar10的触发器和stl10的类相映射，能否使该encoder直接对stl10生成触发器也产生较好的效果？
预训练模型 3个 cifar10 ImageNet CLIP，在这三个数据集上，分别对GTSRB, SVHN, and STL10进行微调，得到backdoor encoder，总共获得9个Backdoor encoder
使用这9个backdoor encoder，对各自微调的数据集进行下游任务训练，最后得到9个结果

触发器就需要做一种，因为对应的目标类别只有一个！
即，我使用触发器只是想把带有触发器的图片的目标修改成指定的类别，而不需要多个类别
不过对于仅仅一个类，也可以衍生出触发器的不同。
例如，使用S="encode"加密所有图片生成residual
在后门攻击时，可以使用一张图+对应的触发器，或者一张图+所有触发器（包含该图片对应的触发器和其他图片的触发器），如果使用所有触发器，可以强化触发器的模板，而非触发器本身。因为太多触发器学习了，导致encoder学到的是所有触发器的共性，而非单一的触发器，这就更增强了后门的泛化性。
但如果使一张图+对应的触发器则encoder学到的更多的是图片和触发器之间关系和目标类别的映射（例如图片的边缘都会产生一个变化，作为触发器。⬅️这种变化被学到了，同时这种变化被映射到了指定类别上）。而前一种encoder学到的更多的使触发器本身的共性，即触发器中像素点值的分布情况。

孰强孰弱不好说，建议都试试。

一张图+对应的触发器：
假如影子数据集有1000张图片，对于每一张图片都有一个触发器，则每张影子样本有1个触发器
对于每个参考图像都需要和1个触发器做对比

一张图+所有触发器：
假如影子数据集有1000张图片，对于每一张图片都有一个触发器，则每张影子样本有1000个触发器
对于每个参考图像都需要和1000个触发器做对比


如果在A数据集上进行微调，在B数据集上进行下游任务，则后门效果不好的话
补充新的实验，即，将A数据集拆分为2部分，在A-1进行微调，植入后门，在A-2进行下游训练，查看后门的准确率。这里由于把完整的数据集拆分了
因此可能会影响良性样本的准确率，但是不碍事，实在是太低了的话就不列出来了，因为拆分数据集后，数据量小了，准确率低是正常的

有2种情况：
1、攻击者为服务的提供方，则攻击者可以接触数据集
    预训练完整cifar10 参考样本：完整cifar10中的第0类 shadow：完整cifar10中的其他数据【需要确定这里的shadow和第0类的数量不一样，需不需要让样本数量一致？】
    则微调后门后，其他类别数据+trigger = 第0类
2、攻击者为第三方，则攻击者不能直接接触数据集
    将cifar10拆分成 A、B、C 3个部分
    先提取 C ,包含1000张（无第0类样本）
    剩下的5000张样本中，拆分成AB
    A预训练用，B作后门训练用，但是后门训练时，有参考样本（纯第0类）和shadow（无第0类）
    先需要确定需不需要让reference和shadow样本数量一致

    模型拥有者用A进行预训练，得到一个clean encoder
    敌手作为第三方使用B，假设B是从网络上搜集的照片，进行模型的微调
    其中B的第0类作为参考样本，其他类作为shadow
    shadow+trigger = 第0类
    测试时，使用C（无第0类样本）作为样本测试后门
    
总体实验如下：

预训练方法           后门攻击微调数据集     下游任务
Fashion-MNIST       Fashion-MNIST         Fashion-MNIST
CIFAR10             CIFAR10               CIFAR10
                    GTSRB                 GTSRB
                    SVHN	              SVHN
                    STL10                 STL10
CIFAR100            CIFAR100              CIFAR100
                    GTSRB                 GTSRB
                    SVHN	              SVHN
                    STL10                 STL10
Mini-ImageNet       Mini-ImageNet         Mini-ImageNet
                    GTSRB                 GTSRB
                    SVHN	              SVHN
                    STL10                 STL10
ImageNet            ImageNet              ImageNet
                    GTSRB                 GTSRB
                    SVHN	              SVHN
                    STL10                 STL10
